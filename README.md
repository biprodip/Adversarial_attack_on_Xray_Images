# Adversarial_attack_on_Xray_Images

An adversarial attack embodies of subtly changing an original image in such a way that the changes are almost imperceptible to the human eye. Hence, the modified image is named an adversarial image that is misclassified by the classifier when it is classified. Since, well known pre-trained models are publicly available, that are used to detect COVID-19 and other similar diseases from radiology images, we formulated FGSM attack for these models. 

